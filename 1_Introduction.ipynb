{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0198e0f4-32d5-4ca4-9142-229ded352774",
   "metadata": {},
   "source": [
    "### 1.1: Self-Play\n",
    "\n",
    "Each agent will approximately converge to a non-exploitable policy. For any state where player 1 has a guaranteed winning strategy available, we will have $V^1(s) = 1, V^2(s) = 0$, and for any state where player 2 has a guaranteed winning stategy, we'll have $V^1(s) = 0, V^2(s) = 1$. For all other states, $V^1(s) \\approx V^2(s) \\approx 0$, since optimal play by both players is likely to result in a draw. However, because each player takes exploratory actions, there is a chance that the opponent responds sub-optimally so we don't have $V(s) = 0$ unless the exploration rate is reduced to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bf027-cdca-40ab-88b5-261a3bc2fc65",
   "metadata": {},
   "source": [
    "### 1.2: Symmetries\n",
    "\n",
    "If we're learning a non-exploitable policy, or a policy we hope will do well against a range of opponents, we might combine all of our value estimates for each symmetrically-equivalent state. One way to do this would be to store a table $V(\\hat{s})$, where $\\hat{s}$ is a \"canonicalized\" state which makes all flips / rotations of $s$ canonicalize to the same $\\hat{s}$. This would speed up learning because every time we visit a state $s$, all symmetrically-equivalent states will have their value estimates $V(\\hat{s})$ updated rather than needing to learning each value separately. In the limit, the values for equivalent states will be the same, so the two will converge to the same answer, but the combined table will converge faster.\n",
    "\n",
    "However, if we are playing against a particular opponent who does not play the same in symmetric positions, then we should not take advantage of symmetries either. For example, if our opponent plays very badly in one board rotation, that rotation should have a higher value for our player than other equivalent positions, and we should try to wind up in that situation. So, thinking of our opponent as part of the environment, different states that are symmetrically equivalent under the rules of tic-tac-toe are not actually equivalent in this environment, and shouldn't have the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195c9a1-cf07-4aad-97b4-94faf8555413",
   "metadata": {},
   "source": [
    "### 1.3: Greedy Play\n",
    "\n",
    "A greedy player might face issues getting \"stuck\" playing a suboptimal strategy. Suppose the RL player initializes $V(s) = 0$ for each $s$, and plays a series of moves where the opponent loses some small percentage of the time. The value for those states will be positive, and so the RL player will continue playing those moves without ever trying other moves. However, the opponent might lose more in other situations, which the RL player will never experience due to under-exploration, so it is likely to converge to a worse policy than the non-greedy player.\n",
    "\n",
    "However, an agent that always plays non-greedily will also always play suboptimally due to exploratory moves. The best bet is probably to explore more in the beginning to ensure the value of every state is well-estimated, and then to transition to playing more greedily over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089443c-35d0-4fcb-b051-08511a7d40cf",
   "metadata": {},
   "source": [
    "### 1.4: Learning from Exploration\n",
    "\n",
    "If we only update after non-exploratory moves, then $V(s)$ will converge to the probability that we win from state $s$, assuming we always take the greedy action in every state afterwards. If we update after every move, then $V(s)$ will instead converge to the probability that our actual policy (including the tendency to explore) will win from state $s$.\n",
    "\n",
    "Assuming we do continue to make exploratory moves, the second set of probabilities will be better to learn, because it will steer our agent towards situations where it in particular is more likely to win (rather than situations where the hypothetical non-greedy player is more likely to win)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82c807-edfd-490a-98a0-9ee5b3783f77",
   "metadata": {},
   "source": [
    "### 1.5: Other Improvements\n",
    "\n",
    "One possible improvement would be reducing the exploration rate over time, so that every state is sufficiently explored but the final agent acts greedily with respect to the state values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
